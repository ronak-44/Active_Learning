# -*- coding: utf-8 -*-
"""Ensemble.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19QYnrrUAoqNThfJ7Kxx4vJJ3jy7h4ioA
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/gdrive')
# %cd "/content/gdrive/My Drive/active"

import pandas as pd
origdata = pd.read_csv("pcmdata.csv",index_col=0)
#origdata['new'] = 0
origdata[:2]

#del origdata['new'] 
#del origdata['activity']
del origdata['target'] 
del origdata['class']
origdata[:2]

# Commented out IPython magic to ensure Python compatibility.
# %cd "master/"

data = origdata.values
X = data[:,:-1]
y = data[:,-1]

from sklearn import preprocessing
normalizer = preprocessing.Normalizer().fit(X)
X = normalizer.transform(X)







# Commented out IPython magic to ensure Python compatibility.
from sklearn.decomposition import PCA
# Define our PCA transformer and fit it onto our raw dataset.
pca = PCA(n_components=1)
transformed = pca.fit_transform(X=X)
# %matplotlib inline
import matplotlib as mpl
import matplotlib.pyplot as plt

# Isolate the data we'll need for plotting.
x_component = transformed[:, 0]

# Plot our dimensionality-reduced (via PCA) dataset.
plt.figure(figsize=(8.5, 6), dpi=130)
plt.scatter(x=x_component, y=y, c=y, cmap='viridis', s=50, alpha=8/10)
plt.title('after PCA transformation')
plt.show()

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline

from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import WhiteKernel, RBF
from modAL.models import ActiveLearner, CommitteeRegressor
from modAL.disagreement import max_std_sampling



# initializing the regressors
n_initial = 100
kernel = RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e3)) \
         + WhiteKernel(noise_level=1, noise_level_bounds=(1e-10, 1e+1))

initial_idx = list()
initial_idx.append(np.random.choice(range(100), size=n_initial, replace=False))
initial_idx.append(np.random.choice(range(100, 200), size=n_initial, replace=False))

learner_list = [ActiveLearner(
                        estimator=GaussianProcessRegressor(kernel),
                        X_training=X[idx], y_training=y[idx]
                )
                for idx in initial_idx]

committee = CommitteeRegressor(
    learner_list=learner_list,
    query_strategy=max_std_sampling
)

# active regression
n_queries = 150
performance_history = []
for idx in range(n_queries):
    query_idx, query_instance = committee.query(X)
    committee.teach(X[query_idx], y[query_idx])
    model_accuracy = committee.learner_list[0].score(X, y)
    print('Accuracy after query {n}: {acc:0.4f}'.format(n=idx + 1, acc=model_accuracy))
    performance_history.append(model_accuracy)

# visualizing the regressors
with plt.style.context('seaborn-white'):
    plt.figure(figsize=(14, 7))
    x = X

    plt.subplot(1, 2, 1)
    for learner_idx, learner in enumerate(committee):
        plt.plot(x_component, learner.predict(x), linewidth=5)
    plt.scatter(x_component, y, c='k')
    plt.title('Regressors after %d queries' % n_queries)

    plt.subplot(1, 2, 2)
    pred, std = committee.predict(x, return_std=True)
    pred = pred.reshape(-1, )
    std = std.reshape(-1, )
    plt.plot(x_component, pred, c='r', linewidth=5)
    plt.fill_between(x_component, pred - std, pred + std, alpha=0.2)
    plt.scatter(x_component, y, c='k')
    plt.title('Prediction of the ensemble after %d queries' % n_queries)
    plt.show()

fig, ax = plt.subplots(figsize=(10, 5))

ax.plot(performance_history)
ax.scatter(range(len(performance_history)), performance_history, s=13)

ax.xaxis.set_major_locator(mpl.ticker.MaxNLocator(nbins=5, integer=True))
ax.yaxis.set_major_locator(mpl.ticker.MaxNLocator(nbins=10))
ax.yaxis.set_major_formatter(mpl.ticker.PercentFormatter(xmax=1))

ax.set_ylim(bottom=0, top=1)
ax.grid(True)

ax.set_title('Incremental classification accuracy')
ax.set_xlabel('Query iteration')
ax.set_ylabel('Classification Accuracy')

plt.show()

from sklearn.metrics import r2_score
r2_score(y, pred)



